{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"7106cc77-d9dd-42df-8b06-851cd9c83a23","state":{"72038116-7847-4c17-a543-4ab1254e2d9a":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"dolore irure","2":"fugiat.laborum@est.com","index":1},{"0":"1","1":"excepteur dolor","2":"in.minim@ut.com","index":2},{"0":"2","1":"ut enim","2":"aute.excepteur@esse.com","index":3},{"0":"3","1":"NULL","2":"commodo.sunt@tempor.com","index":4},{"0":"4","1":"NULL","2":"ut.id@commodo.com","index":5},{"0":"5","1":"enim incididunt","2":"nisi.fugiat@amet.com","index":6},{"0":"6","1":"non elit","2":"nisi.irure@esse.com","index":7},{"0":"7","1":"commodo duis","2":"ut.id@ut.com","index":8},{"0":"8","1":"id eu","2":"amet.ea@aliquip.com","index":9},{"0":"9","1":"NULL","2":"dolor.sunt@excepteur.com","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"email","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"72038116-7847-4c17-a543-4ab1254e2d9a":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"dolore irure","2":"fugiat.laborum@est.com","index":1},{"0":"1","1":"excepteur dolor","2":"in.minim@ut.com","index":2},{"0":"2","1":"ut enim","2":"aute.excepteur@esse.com","index":3},{"0":"3","1":"NULL","2":"commodo.sunt@tempor.com","index":4},{"0":"4","1":"NULL","2":"ut.id@commodo.com","index":5},{"0":"5","1":"enim incididunt","2":"nisi.fugiat@amet.com","index":6},{"0":"6","1":"non elit","2":"nisi.irure@esse.com","index":7},{"0":"7","1":"commodo duis","2":"ut.id@ut.com","index":8},{"0":"8","1":"id eu","2":"amet.ea@aliquip.com","index":9},{"0":"9","1":"NULL","2":"dolor.sunt@excepteur.com","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"email","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"aa79825f-52d4-4b66-8452-34ede639ba5d"}],"default_lakehouse":"aa79825f-52d4-4b66-8452-34ede639ba5d","default_lakehouse_name":"msdatatesting","default_lakehouse_workspace_id":"2249f67c-347d-4752-b453-ff5473c55ff3"}}},"cells":[{"cell_type":"code","source":["# install the dbldatagen library\n","%pip install dbldatagen\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f95a3ba2-653f-4414-833d-078fbdb7d1f6","statement_id":36,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T19:16:54.8580703Z","session_start_time":null,"execution_start_time":"2023-10-09T19:17:05.1268318Z","execution_finish_time":"2023-10-09T19:17:07.9966114Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0372ebf0-fbd8-4869-b146-7f29c6ab1c94"},"text/plain":"StatementMeta(, f95a3ba2-653f-4414-833d-078fbdb7d1f6, 36, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dbldatagen in /nfs4/pyenv-643e721b-4f12-4aa5-8d3b-65750da265a5/lib/python3.10/site-packages (0.3.5)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-643e721b-4f12-4aa5-8d3b-65750da265a5/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":11,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":11,"metadata":{},"id":"8f4ec457-c13c-48fc-ae04-f18cd3ce1a74"},{"cell_type":"code","source":["# Fourth Synthetic data generation example\n","\n","import dbldatagen as dg #import the library\n","from pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n","\n","ds = (\n","     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n","     .withIdOutput() #Create an ID Column\n","     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\\\w \\\\w') #create a name based on a template with 30% of rows as NULLs\n","     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n",")\n","\n","df = ds.build() #here the data is actually generated and the DataFrame is built\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\n","display(df.limit(10)) # show 10 rows of data#"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f95a3ba2-653f-4414-833d-078fbdb7d1f6","statement_id":44,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T19:21:48.6630479Z","session_start_time":null,"execution_start_time":"2023-10-09T19:21:49.0046228Z","execution_finish_time":"2023-10-09T19:21:58.1682731Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":6,"RUNNING":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4342,"rowCount":50,"usageDescription":"","jobId":46,"name":"toString at String.java:2994","description":"Delta: Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#: Compute snapshot for version: 3","submissionTime":"2023-10-09T19:21:55.521GMT","completionTime":"2023-10-09T19:21:55.553GMT","stageIds":[70,71,69],"jobGroup":"44","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":54,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4342,"dataRead":6657,"rowCount":63,"usageDescription":"","jobId":45,"name":"toString at String.java:2994","description":"Delta: Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#: Compute snapshot for version: 3","submissionTime":"2023-10-09T19:21:54.850GMT","completionTime":"2023-10-09T19:21:55.497GMT","stageIds":[67,68],"jobGroup":"44","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6657,"dataRead":4823,"rowCount":26,"usageDescription":"","jobId":44,"name":"toString at String.java:2994","description":"Delta: Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#: Compute snapshot for version: 3","submissionTime":"2023-10-09T19:21:53.479GMT","completionTime":"2023-10-09T19:21:53.781GMT","stageIds":[66],"jobGroup":"44","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":2141,"rowCount":5,"usageDescription":"","jobId":43,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#","submissionTime":"2023-10-09T19:21:52.580GMT","completionTime":"2023-10-09T19:21:52.691GMT","stageIds":[64,65],"jobGroup":"44","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":3044,"dataRead":4146,"rowCount":200,"usageDescription":"","jobId":42,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#","submissionTime":"2023-10-09T19:21:51.770GMT","completionTime":"2023-10-09T19:21:52.477GMT","stageIds":[63,62],"jobGroup":"44","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":4146,"dataRead":0,"rowCount":200,"usageDescription":"","jobId":41,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 44:\n# Fourth Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=100, partitions=4)#100 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.3, template=r'\\w \\w') #create a name based on a template with 30% of rows as NULLs\n     .withColumn(\"email\", StringType(), template=r'\\w.\\w@\\w.com') #create an email based on a template\n)\n\ndf = ds.build() #here the data is actually generated and the DataFrame is built\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example4\")\ndisplay(df.limit(10)) # show 10 rows of data#","submissionTime":"2023-10-09T19:21:50.805GMT","completionTime":"2023-10-09T19:21:51.728GMT","stageIds":[61],"jobGroup":"44","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"19c2bf70-baf1-448c-8368-666d0a669f60"},"text/plain":"StatementMeta(, f95a3ba2-653f-4414-833d-078fbdb7d1f6, 44, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"72038116-7847-4c17-a543-4ab1254e2d9a","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 72038116-7847-4c17-a543-4ab1254e2d9a)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"03b7b90a-e53d-4309-96bf-1c270e726e6e"}]}