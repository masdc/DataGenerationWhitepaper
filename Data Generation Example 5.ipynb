{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"a7fc20a0-08c0-42b5-a637-f7bdaa972f25","state":{"036f1169-c006-442c-b73e-8e35f373005d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Holly Miller","2":"crystalthomas@hayes-pope.com","index":1},{"0":"1","1":"Mark Torres","2":"nelsonkristen@yu.com","index":2},{"0":"2","1":"Denise Curry","2":"katiejones@jensen.com","index":3},{"0":"3","1":"Sara Boone","2":"robertwilliams@webb.com","index":4},{"0":"4","1":"Eric Peters","2":"william05@durham-hart.com","index":5},{"0":"5","1":"Ivan Carpenter","2":"santosjohn@nielsen.com","index":6},{"0":"6","1":"Penny Osborn","2":"asimpson@whitney.com","index":7},{"0":"7","1":"Dustin Fernandez","2":"jonathan62@kelley.net","index":8},{"0":"8","1":"Michael Reed","2":"sarahwilkins@fuller.com","index":9},{"0":"9","1":"Tracie Pham","2":"patricia77@martin-blair.net","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"email","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"036f1169-c006-442c-b73e-8e35f373005d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Holly Miller","2":"crystalthomas@hayes-pope.com","index":1},{"0":"1","1":"Mark Torres","2":"nelsonkristen@yu.com","index":2},{"0":"2","1":"Denise Curry","2":"katiejones@jensen.com","index":3},{"0":"3","1":"Sara Boone","2":"robertwilliams@webb.com","index":4},{"0":"4","1":"Eric Peters","2":"william05@durham-hart.com","index":5},{"0":"5","1":"Ivan Carpenter","2":"santosjohn@nielsen.com","index":6},{"0":"6","1":"Penny Osborn","2":"asimpson@whitney.com","index":7},{"0":"7","1":"Dustin Fernandez","2":"jonathan62@kelley.net","index":8},{"0":"8","1":"Michael Reed","2":"sarahwilkins@fuller.com","index":9},{"0":"9","1":"Tracie Pham","2":"patricia77@martin-blair.net","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"email","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"aa79825f-52d4-4b66-8452-34ede639ba5d"}],"default_lakehouse":"aa79825f-52d4-4b66-8452-34ede639ba5d","default_lakehouse_name":"msdatatesting","default_lakehouse_workspace_id":"2249f67c-347d-4752-b453-ff5473c55ff3"}}},"cells":[{"cell_type":"code","source":["# install the dbldatagen library\n","%pip install dbldatagen\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"db3954e6-028b-486e-84e6-b6937f198b50","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T19:56:09.8435538Z","session_start_time":null,"execution_start_time":"2023-10-09T19:56:46.131853Z","execution_finish_time":"2023-10-09T19:56:47.9877158Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ef90dbb5-40a9-40d0-bdd8-9b8c99dbb3a8"},"text/plain":"StatementMeta(, db3954e6-028b-486e-84e6-b6937f198b50, 8, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting dbldatagen\n  Downloading dbldatagen-0.3.5-py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.3.5\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-4e242fe0-14b6-482c-a48d-532eba0f9d88/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{},"id":"8f4ec457-c13c-48fc-ae04-f18cd3ce1a74"},{"cell_type":"code","source":["# install the Faker library\n","%pip install Faker"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"db3954e6-028b-486e-84e6-b6937f198b50","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T19:56:09.8441667Z","session_start_time":null,"execution_start_time":"2023-10-09T19:57:01.3640993Z","execution_finish_time":"2023-10-09T19:57:03.1992428Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"810b8bae-536c-41c8-b785-fe6ff7bcc21a"},"text/plain":"StatementMeta(, db3954e6-028b-486e-84e6-b6937f198b50, 15, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting Faker\n  Downloading Faker-19.8.0-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from Faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\nInstalling collected packages: Faker\nSuccessfully installed Faker-19.8.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-4e242fe0-14b6-482c-a48d-532eba0f9d88/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":2,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"81c3cd79-373a-42e9-af4c-f17d1813e9bd"},{"cell_type":"code","source":["# Fifth Synthetic data generation exercice\n","\n","import dbldatagen as dg #import the library\n","from faker import Faker #import the library\n","from pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n","from dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n","\n","\n","name_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object. \n","This function will create the Fake Name\n","email_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n","\n","ds = (\n","     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n","     .withIdOutput() #Create an ID Column\n","     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column as \"name\", make it a string and 10% of names will be null.  Uses the Labda function created above to generate the fake name\n","     .withColumn(\"email\", StringType(), text=PyfuncText(email_generator)) #create a fake email , name the column as \"email\" and use the Lambda function created above to generate the fake email\n",")\n","\n","df = ds.build() #here the data is actually generated and the DataFrame is built\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example5\") #save the DataFrame as a Table in the Lakehouse\n","display(df.limit(10)) # show 10 rows of data#"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"db3954e6-028b-486e-84e6-b6937f198b50","statement_id":18,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T19:57:23.1223152Z","session_start_time":null,"execution_start_time":"2023-10-09T19:57:23.4749218Z","execution_finish_time":"2023-10-09T19:57:46.2018851Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":7},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":10,"usageDescription":"","jobId":15,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...","submissionTime":"2023-10-09T19:57:43.687GMT","completionTime":"2023-10-09T19:57:43.918GMT","stageIds":[22],"jobGroup":"18","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4324,"rowCount":50,"usageDescription":"","jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...: Compute snapshot for version: 0","submissionTime":"2023-10-09T19:57:41.679GMT","completionTime":"2023-10-09T19:57:41.749GMT","stageIds":[19,20,21],"jobGroup":"18","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4324,"dataRead":1609,"rowCount":54,"usageDescription":"","jobId":13,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...: Compute snapshot for version: 0","submissionTime":"2023-10-09T19:57:40.658GMT","completionTime":"2023-10-09T19:57:41.652GMT","stageIds":[17,18],"jobGroup":"18","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1609,"dataRead":1412,"rowCount":8,"usageDescription":"","jobId":12,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...: Compute snapshot for version: 0","submissionTime":"2023-10-09T19:57:38.420GMT","completionTime":"2023-10-09T19:57:38.586GMT","stageIds":[16],"jobGroup":"18","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":11,"name":"","description":"Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...","submissionTime":"2023-10-09T19:57:37.296GMT","completionTime":"2023-10-09T19:57:37.296GMT","stageIds":[],"jobGroup":"18","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":2219,"dataRead":816,"rowCount":20,"usageDescription":"","jobId":10,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...","submissionTime":"2023-10-09T19:57:36.083GMT","completionTime":"2023-10-09T19:57:37.123GMT","stageIds":[15,14],"jobGroup":"18","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":816,"dataRead":0,"rowCount":20,"usageDescription":"","jobId":9,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 18:\n# Fifth Synthetic data generation exercice\n\nimport dbldatagen as dg #import the library\nfrom faker import Faker #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\nfrom dbldatagen import PyfuncText#this will help us calling the Faker library from within the DataGenerator object\n\n\nname_generator = (lambda context, v : Faker(locale=\"en_US\").name()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Name\nemail_generator = (lambda context, v : Faker(locale=\"en_US\").ascii_company_email()) #create a lambda function to be called from the DataGeneration object.  This function will create the Fake Email\n\nds = (\n     dg.DataGenerator(sparkSession=spark, name=\"test_dataset1\", rows=10, partitions=4) #10 rows of data\n     .withIdOutput() #Create an ID Column\n     .withColumn(\"name\", StringType(), percentNulls=0.1, text=PyfuncText(name_generator)) #create a fake name, name the column a...","submissionTime":"2023-10-09T19:57:35.695GMT","completionTime":"2023-10-09T19:57:36.043GMT","stageIds":[13],"jobGroup":"18","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"af42b958-0862-4e57-b3b0-e266506c11ab"},"text/plain":"StatementMeta(, db3954e6-028b-486e-84e6-b6937f198b50, 18, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"036f1169-c006-442c-b73e-8e35f373005d","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 036f1169-c006-442c-b73e-8e35f373005d)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"0037e55c-b111-4aa6-836d-7c3fa60e0eed"}]}