{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"7cabc9d5-296e-429c-a41f-9578d5f660af","state":{"3a5eb158-651d-48bb-816c-b333593d3940":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"197","2":"7","3":"a","4":"c","index":1},{"0":"1","1":"147","2":"8","3":"b","4":"a","index":2},{"0":"2","1":"109","2":"5","3":"c","4":"b","index":3},{"0":"3","1":"174","2":"9","3":"b","4":"c","index":4},{"0":"4","1":"179","2":"3","3":"b","4":"b","index":5},{"0":"5","1":"128","2":"4","3":"b","4":"c","index":6},{"0":"6","1":"176","2":"6","3":"a","4":"b","index":7},{"0":"7","1":"181","2":"2","3":"b","4":"c","index":8},{"0":"8","1":"122","2":"2","3":"c","4":"a","index":9},{"0":"9","1":"191","2":"7","3":"b","4":"b","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"code1","type":"int"},{"key":"2","name":"code2","type":"int"},{"key":"3","name":"code3","type":"string"},{"key":"4","name":"code4","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["3"],"seriesFieldKeys":["1"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"3a5eb158-651d-48bb-816c-b333593d3940":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"197","2":"7","3":"a","4":"c","index":1},{"0":"1","1":"147","2":"8","3":"b","4":"a","index":2},{"0":"2","1":"109","2":"5","3":"c","4":"b","index":3},{"0":"3","1":"174","2":"9","3":"b","4":"c","index":4},{"0":"4","1":"179","2":"3","3":"b","4":"b","index":5},{"0":"5","1":"128","2":"4","3":"b","4":"c","index":6},{"0":"6","1":"176","2":"6","3":"a","4":"b","index":7},{"0":"7","1":"181","2":"2","3":"b","4":"c","index":8},{"0":"8","1":"122","2":"2","3":"c","4":"a","index":9},{"0":"9","1":"191","2":"7","3":"b","4":"b","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"code1","type":"int"},{"key":"2","name":"code2","type":"int"},{"key":"3","name":"code3","type":"string"},{"key":"4","name":"code4","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["3"],"seriesFieldKeys":["1"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"aa79825f-52d4-4b66-8452-34ede639ba5d"}],"default_lakehouse":"aa79825f-52d4-4b66-8452-34ede639ba5d","default_lakehouse_name":"msdatatesting","default_lakehouse_workspace_id":"2249f67c-347d-4752-b453-ff5473c55ff3"}}},"cells":[{"cell_type":"code","source":["# install the dbldatagen library\n","%pip install dbldatagen\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"32c23642-c78a-4373-89b1-c10f08606904","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T18:39:37.7165185Z","session_start_time":null,"execution_start_time":"2023-10-09T18:40:06.1866596Z","execution_finish_time":"2023-10-09T18:40:09.2256398Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d45a6e20-42f2-4aa9-90c2-b53b1d164c09"},"text/plain":"StatementMeta(, 32c23642-c78a-4373-89b1-c10f08606904, 8, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting dbldatagen\n  Downloading dbldatagen-0.3.5-py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.3.5\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-3f2f4466-b6e3-462e-83ee-1645259c035e/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{},"id":"8f4ec457-c13c-48fc-ae04-f18cd3ce1a74"},{"cell_type":"code","source":["# Third Synthetic data generation example\n","\n","import dbldatagen as dg #import the library\n","from pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n","\n","\n","\n","ds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n","                            .withIdOutput() #Create an ID Column\n","                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n","                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n","                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n","                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code4\" of String Type, with String Type, with characters a,b or c as values\n","                            )\n","                            \n","df = ds.build() #here the data is actually generated and the DataFrame is built\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Example3\") #save the DataFrame as a Table in the Lakehouse\n","display(df.limit(10)) # show 10 rows of data"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"32c23642-c78a-4373-89b1-c10f08606904","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-09T18:42:42.1529976Z","session_start_time":null,"execution_start_time":"2023-10-09T18:42:42.5362885Z","execution_finish_time":"2023-10-09T18:42:49.9876855Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":10,"usageDescription":"","jobId":21,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...","submissionTime":"2023-10-09T18:42:49.561GMT","completionTime":"2023-10-09T18:42:49.583GMT","stageIds":[33],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4346,"rowCount":50,"usageDescription":"","jobId":20,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...: Compute snapshot for version: 1","submissionTime":"2023-10-09T18:42:48.299GMT","completionTime":"2023-10-09T18:42:48.339GMT","stageIds":[30,31,32],"jobGroup":"11","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4346,"dataRead":3282,"rowCount":57,"usageDescription":"","jobId":19,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...: Compute snapshot for version: 1","submissionTime":"2023-10-09T18:42:47.542GMT","completionTime":"2023-10-09T18:42:48.276GMT","stageIds":[28,29],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3282,"dataRead":2756,"rowCount":14,"usageDescription":"","jobId":18,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...: Compute snapshot for version: 1","submissionTime":"2023-10-09T18:42:46.656GMT","completionTime":"2023-10-09T18:42:46.919GMT","stageIds":[27],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1744,"rowCount":3,"usageDescription":"","jobId":17,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...","submissionTime":"2023-10-09T18:42:45.439GMT","completionTime":"2023-10-09T18:42:45.685GMT","stageIds":[25,26],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":2544,"dataRead":519,"rowCount":20,"usageDescription":"","jobId":16,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...","submissionTime":"2023-10-09T18:42:44.528GMT","completionTime":"2023-10-09T18:42:45.244GMT","stageIds":[24,23],"jobGroup":"11","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":519,"dataRead":0,"rowCount":20,"usageDescription":"","jobId":15,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n# Third Synthetic data generation example\n\nimport dbldatagen as dg #import the library\nfrom pyspark.sql.types import IntegerType, FloatType, StringType  #use sql types to define the type of the columns.\n\n\n\nds = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=10,partitions=4) #10 rows of data\n                            .withIdOutput() #Create an ID Column\n                            .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200, random=True) #create a column named \"code1\" of Integer Type, with values from 100 to 200\n                            .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10, random=True) #create a column named \"code2\" of Integer Type, with values from 0 to 20\n                            .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named \"code3\" of String Type, with characters a,b or c as values\n                            .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], random=True) #create a column named ...","submissionTime":"2023-10-09T18:42:44.431GMT","completionTime":"2023-10-09T18:42:44.475GMT","stageIds":[22],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a4857275-733b-42ef-bb76-5859135f7d01"},"text/plain":"StatementMeta(, 32c23642-c78a-4373-89b1-c10f08606904, 11, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3a5eb158-651d-48bb-816c-b333593d3940","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3a5eb158-651d-48bb-816c-b333593d3940)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"2a1eed6c-5844-4d4e-a565-a3b3b3fe71dc"}]}